{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Prompt Engineering\n",
    "# Date Created: 03/07/23\n",
    "\n",
    "# Load in packages\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import tiktoken\n",
    "from evaluation import evaluate, get_results_dict, save_results\n",
    "from helper_functions import check_dir_exists, load_n_samples, load_balanced_n_samples, convert_labels\n",
    "import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5131e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in datasets\n",
    "binary_abuse =pd.read_csv(\"../data/binary_abuse/clean_data/binary_abuse_dev_sample.csv\")\n",
    "imdb = pd.read_csv(\"../data/binary_movie_sentiment/clean_data/binary_movie_sentiment_dev_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://openai.com/pricing#language-models\n",
    "# GPT-3.5 Turbo (as of 18/07/2023)\n",
    "# 4K context   $0.0015 / 1K tokens  $0.002 / 1K tokens\n",
    "# 16K context  $0.003 / 1K tokens   $0.004 / 1K tokens\n",
    "\n",
    "# https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/?cdn=disable\n",
    "# Models           Per 1,000 tokens\n",
    "# gpt-3.5-turbo    Â£0.001586\n",
    "\n",
    "# iterate over all to get the number of tokens - imdb\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\") # shuold be the tokenizer for gpt3.5\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(imdb[\"text\"])):\n",
    "    sample_text=imdb[\"text\"][i]\n",
    "    decoded_text = tokenizer.decode_tokens_bytes(tokenizer.encode(sample_text))\n",
    "    token_lengths.append(len(decoded_text))\n",
    "\n",
    "print(\"IMDB: N=\",len(token_lengths),\"; Max Token Len=\",max(token_lengths),\"; Min Token Len=\",min(token_lengths),\"; Avg Token Len=\",sum(token_lengths)/len(token_lengths))\n",
    "print(\"\\n\")\n",
    "print(imdb[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "# IMDB: N= 1250 ; Max Token Len= 1407 ; Min Token Len= 8 ; Avg Token Len= 274.024\n",
    "\n",
    "\n",
    "# False    625\n",
    "# True     625\n",
    "# Name: label, dtype: int64\n",
    "# CPU times: total: 5.05 s\n",
    "# Wall time: 5.18 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# iterate over all to get the number of tokens -binary_abuse\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\") # shuold be the tokenizer for gpt3.5\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(binary_abuse[\"text\"])):\n",
    "    sample_text=binary_abuse[\"text\"][i]\n",
    "    decoded_text = tokenizer.decode_tokens_bytes(tokenizer.encode(sample_text))\n",
    "    token_lengths.append(len(decoded_text))\n",
    "\n",
    "print(\"Wiki Abuse: N=\",len(token_lengths),\"; Max Token Len=\",max(token_lengths),\"; Min Token Len=\",min(token_lengths),\"; Avg Token Len=\",sum(token_lengths)/len(token_lengths))\n",
    "print(\"\\n\")\n",
    "print(binary_abuse[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "# Wiki Abuse: N= 2316 ; Max Token Len= 2858 ; Min Token Len= 2 ; Avg Token Len= 95.39896373056995\n",
    "\n",
    "\n",
    "# False    2041\n",
    "# True      275\n",
    "# Name: label, dtype: int64\n",
    "# CPU times: total: 3.61 s\n",
    "# Wall time: 3.66 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Access via text completion\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "base_url = \"https://tos-openai.openai.azure.com/\"\n",
    "\n",
    "deployment_name =\"tos_gpt35\" # gpt 3.5 final model for paper\n",
    "url = base_url + \"/openai/deployments/\" + deployment_name + \"/completions?api-version=2023-05-15\"\n",
    "\n",
    "\n",
    "def get_openai_response(prompt):\n",
    "    \"\"\"Gets a response from the OpenAI API from a given prompt\n",
    "    Args:\n",
    "        prompt: a text prompt to send to the API\n",
    "    Returns:\n",
    "        a string response from the API\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"prompt\":prompt,\n",
    "        \"max_tokens\":20,\n",
    "        \"top_p\":.1,\n",
    "        #\"stop\":[\" \",\".\",\",\"] # ideally only want one word responses\n",
    "        \"stop\":[\".\",\"\\n\"] # get one sentence responses\n",
    "    }\n",
    "    \n",
    "    r = requests.post(url, \n",
    "                      headers={\n",
    "                          \"api-key\": api_key,\n",
    "                          \"Content-Type\": \"application/json\"\n",
    "                      },\n",
    "                      json = payload\n",
    "                     )\n",
    "    response = json.loads(r.text)\n",
    "    if r.status_code!=200:\n",
    "        #raise Exception(r.json().get(\"error\").get(\"message\"))\n",
    "        text_return = r.json().get(\"error\").get(\"message\")\n",
    "    \n",
    "    if r.status_code==200:\n",
    "        text_return = response['choices'][0]['text']\n",
    "        text_return = text_return.replace('\"', '') # remove \n",
    "        text_return = text_return.replace('\\'', '') # remove \n",
    "    \n",
    "        if text_return==\"Negative\" or text_return==\"Abusive\":\n",
    "            text_return=\"Yes\"        \n",
    "        if text_return==\"Not\" or text_return==\"Positive\":\n",
    "            text_return=\"No\"\n",
    "        \n",
    "    return text_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Access via chat - 18/07/23 switching to this as gpt3.5 turbo seems optimized for chat over completions\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = \"https://tos-openai.openai.azure.com/\"\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "deployment_name =\"tos_gpt35\" # cheat gpt text model for testing\n",
    "\n",
    "def get_openai_response_chat(prompt, text):\n",
    "    \"\"\"Gets a response from the OpenAI API from a given prompt\n",
    "    Args:\n",
    "        prompt: a text prompt to send to the API\n",
    "    Returns:\n",
    "        a string response from the API\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=deployment_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=.1,\n",
    "        max_tokens=10,\n",
    "        stop=[\".\"]\n",
    "    )\n",
    "    \n",
    "    text_return = response['choices'][0]['message']['content']\n",
    "    \n",
    "    \n",
    "    return text_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempted prompts\n",
    "# p: \"\\n\\nDoes this text contain abuse, Yes or No?\\n\\n\"\n",
    "# r: 'I\\'m not sure what you mean by \"abuse'\n",
    "# p: \"\\n\\nDoes this text contain abusive language, Yes or No?\\n\\n\"\n",
    "# r: No\n",
    "# p: \"\\n\\nDoes this text contain abusive language? Respond with either Yes or No.\\n\\n\"\n",
    "# r: 'No'\n",
    "# p: \\n\\nIs this text abusive, Yes or No?\\n\\n\n",
    "# r: I think this question violates the Terms of Service\n",
    "# p: \\n\\nIs this text using abusive language, Yes or No?\\n\\n\n",
    "# r: 'No'\n",
    "# p: \\n\\nIt was? Abusive or Not Abusive?\\n\\n\n",
    "# r: \"I'm not sure what you're asking\"\n",
    "# p: \\n\\nIt was? Abusive Language or Not Abusive Language?\\n\\n\n",
    "# r: I'm not sure what you're askin\n",
    "# p: \\n\\nDoes the above user comment contain abuse? Yes or No. \\n\\n\n",
    "# r: 'Posted by: Cato | 2005'\n",
    "# p: \"\\n\\nDoes the above user comment contain abuse, Yes or No? \\n\\n\"\n",
    "# r: 'No\n",
    "# p: \\n\\nAbusive or Not Abusive? \\n\\n\n",
    "# r: 'Posted by: The Brickmuppet at 11:47 PM | Comments (1) | Add'\n",
    "# p: \\n\\nAbusive or Not Abusive? Which better characterizes the above comment?\\n\\n\n",
    "# r: I'm not sure what you're asking\n",
    "# p: Was that comment Abusive or Not Abusive?\n",
    "# r: Enter your comment here\n",
    "# p: \\n\\nDoes the above user comment contain abusive language, Yes or No? \\n\\n\n",
    "# r: No\n",
    "# p: You are a researcher evaluating abusive language in Wikipedia comments. Does the above comment contain abusive language, Yes or No? \n",
    "# r: 'No'\n",
    "\n",
    "# p:\\n\\nDoes this text contain negative sentiment, Yes or No?\\n\\n\n",
    "# r: No\n",
    "# p:\\n\\nDoes this text contain negative sentiment, Yes or No? \\n\\n\n",
    "# r: Yes\n",
    "# p:\\n\\nDoes the above text contain negative sentiment, Yes or No? \\n\\n\n",
    "# r: Yes\n",
    "# p: \\n\\nIs this text negative, Yes or No?\\n\\n\n",
    "# r:Was the above review useful to you?\n",
    "# p: \\n\\nIt was? Negative or Not Negative?\\n\\n\n",
    "# r: I'm not sure what to say about this movie\n",
    "# p:\\n\\nDoes this text contain negative sentiment, Yes or No?\\n\\n\n",
    "# r: \\n\\nClassify the sentiment of the above text using Positive or Negative.\\n\\nThe above text is \n",
    "# p: ' Negative'\n",
    "# r: \\n\\nWhat is the sentiment of the above text using Positive or Negative.\\n\\nThe above text is \n",
    "# p: ' Negative'\n",
    "# r: \"Using Positive or Negative, classify the sentiment of the following movie review.\\n\\n .... \\n\\nThe sentiment of the movie review is\n",
    "# p:' Negative'\n",
    "\n",
    "\n",
    "binary_abuse_prompt_1 = \"\\n\\nDoes this text contain abuse, Yes or No?\\n\\n\"\n",
    "sample_text=imdb[\"text\"][1]\n",
    "prompt = sample_text + binary_abuse_prompt_2\n",
    "print(prompt)\n",
    "\n",
    "get_openai_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbf2e1",
   "metadata": {},
   "source": [
    "# Final Zero Shot Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set global vars\n",
    "TECH = \"zero_shot_prompt_engineering\"\n",
    "TASK = \"binary_abuse\"\n",
    "\n",
    "n_eval=-1\n",
    "eval_set=\"dev_sample\"\n",
    "model_name = \"gpt3.5\"\n",
    "run_time = \"0\" # no training\n",
    "n_train= \"0\" # no training\n",
    "seed= \"0\" # no training\n",
    "balanced_train=False\n",
    "\n",
    "# Set dirs\n",
    "main_dir = os.getcwd()\n",
    "data_dir = f\"../data\"\n",
    "output_dir = f'../results/{TASK}/{TECH}'\n",
    "\n",
    "# TASK = \"binary_abuse\"\n",
    "# True = Abusive; False = Not Abusive\n",
    "binary_abuse_prompt_1 = \"Using one word, does the internet comment contain toxic language, Yes or No?\"\n",
    "binary_abuse_prompt_2 = \"Using one word, is this internet comment using toxic language, Yes or No?\"\n",
    "binary_abuse_prompt_3 = \"You are a researcher who needs to classify comments on the internet as containing abusive language or not containing abusive language. Using one word, does the internet comment contain abusive language, Yes or No?\"\n",
    " \n",
    "# TASK = \"binary_movie_sentiment\"\n",
    "# True= Positive; False= Negative\n",
    "imdb_prompt_1 = \"Using one word, does the movie review contain negative sentiment, Yes or No?\\n\\n\"\n",
    "imdb_prompt_2 = \"Using one word, classify the sentiment of the movie review using 'Positive' or 'Negative'.\"\n",
    "imdb_prompt_3 = \"You are a researcher who needs to classify movie reviews as containing negative sentiment or not containing negative sentiment. Using one word, does the movie review contain negative sentiment, Yes or No?\"\n",
    "\n",
    "# Lists for loop\n",
    "task_list = [\"binary_abuse\",\"binary_abuse\",\"binary_abuse\",\"binary_movie_sentiment\",\"binary_movie_sentiment\",\"binary_movie_sentiment\"]\n",
    "prompt_list = [binary_abuse_prompt_1,binary_abuse_prompt_2,binary_abuse_prompt_3,imdb_prompt_1,imdb_prompt_2,imdb_prompt_3]\n",
    "\n",
    "####################################################################\n",
    "eval_pred_all = [] # the master lists\n",
    "eval_gold_all = [] # the master lists\n",
    "for t in range(6):\n",
    "    task_prompt = prompt_list[t]\n",
    "    TASK = task_list[t]\n",
    "    temp_df = binary_abuse\n",
    "    if TASK == \"binary_movie_sentiment\":\n",
    "        temp_df=imdb\n",
    "    # iterate over the rows\n",
    "    #for i in range(len(imdb[\"text\"])):\n",
    "    eval_gold_labels=[] # the t/p lists\n",
    "    eval_pred=[] # the t/p lists\n",
    "    for i in range(len(temp_df)):\n",
    "        # grab the comment\n",
    "        sample_text=temp_df[\"text\"][i]\n",
    "        \n",
    "        # make it into a usable user chat\n",
    "        if TASK == \"binary_movie_sentiment\":\n",
    "            temp_text = \"Movie review: \" + sample_text\n",
    "            \n",
    "        if TASK == \"binary_abuse\":\n",
    "            temp_text = \"Internet comment: \" + sample_text\n",
    "        \n",
    "        # append gpt response to list\n",
    "        while True:\n",
    "            try:\n",
    "                eval_pred.append(get_openai_response_chat(task_prompt,temp_text))\n",
    "            except: # this only work if the api response fails due to a ServiceUnavailableError mb try except openai.error.ServiceUnavailableError\n",
    "                time.sleep(15) # if service is not available, wait a few moments and try again\n",
    "                eval_pred.append(get_openai_response_chat(task_prompt,temp_text))\n",
    "                break\n",
    "            break   \n",
    "        # append labels to list\n",
    "        eval_gold_labels.append(temp_df[\"label\"][i])\n",
    "        \n",
    "        time.sleep(.75) # we should be fine vis-a-vis rate limits, but adding a small delay to help prevent any issues with ServiceUnavailableErrors\n",
    "        if i % 50 == 0:\n",
    "            print(t,i)\n",
    "    \n",
    "    eval_pred_all.append(eval_pred)\n",
    "    \n",
    "    # Need to remember that these are flipped \n",
    "    if TASK == \"binary_abuse\":\n",
    "        eval_pred_int=list(pd.Series(eval_pred).map(dict(Yes=1, No=0)))\n",
    "    \n",
    "    # switch my pos/neg to match the other imdb prompts\n",
    "    if task_prompt == imdb_prompt_2:\n",
    "        eval_pred=list(pd.Series(eval_pred).map(dict(Negative=\"Yes\", Positive=\"No\")))\n",
    "        \n",
    "    if TASK == \"binary_movie_sentiment\":\n",
    "        eval_pred_int=list(pd.Series(eval_pred).map(dict(Yes=0, No=1)))\n",
    "    \n",
    "    # Deal with non yes/no answers\n",
    "    print(\"Number of improperly coded responses: \",sum(math.isnan(x) for x in eval_pred_int))\n",
    "    \n",
    "    cleaned_eval_pred_int = [x for x in eval_pred_int if not math.isnan(x)]\n",
    "    cleaned_eval_gold_labels = [eval_gold_labels[i] for i, x in enumerate(eval_pred_int) if not math.isnan(x)]\n",
    "    \n",
    "    \n",
    "    cleaned_eval_gold_labels=list(map(int,cleaned_eval_gold_labels))\n",
    "    \n",
    "    eval_gold_all.append(cleaned_eval_gold_labels)\n",
    "    \n",
    "    results = evaluate(cleaned_eval_gold_labels, cleaned_eval_pred_int)\n",
    "    \n",
    "\n",
    "    print(TASK)\n",
    "    print(task_prompt)\n",
    "    print(results)\n",
    "    \n",
    "    eval_gold_labels=cleaned_eval_gold_labels\n",
    "    eval_preds=cleaned_eval_pred_int\n",
    "    eval_result=results\n",
    "    template=task_prompt\n",
    "\n",
    "    \n",
    "    datetime_str = str(datetime.datetime.now())\n",
    "    results_dict = get_results_dict(TASK, TECH, model_name, run_time,\n",
    "                    eval_gold_labels, eval_preds, eval_set,\n",
    "                    n_train, n_eval, balanced_train, seed, datetime_str, template)\n",
    "    # add test_result to results_dict\n",
    "    results_dict.update(eval_result)\n",
    "    save_str = f'mod={model_name}_n={n_train}_bal={balanced_train}_iteration={t}'\n",
    "    save_results(output_dir, save_str, results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of Non-Response/Incorrect Response\n",
    "\n",
    "# I cannot determine if the internet comment contains toxic language\n",
    "# I'm sorry, but I cannot determine whether the internet comment contains toxic language without the actual comment\n",
    "# N/A (not a comment)\n",
    "# I cannot determine if the internet comment contains toxic language as it appears to be incomplete and lacks context\n",
    "# Cannot determine as there is no internet comment provided\n",
    "# I cannot determine if the internet comment contains toxic language\n",
    "# Personal attack\n",
    "# I cannot make a judgement without seeing the actual internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39578b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Load the GPT predictions\n",
    "#np.save(\"../results/binary_abuse/zero_shot_prompt_engineering/preds.npy\",np.array([np.array(x, dtype=\"object\") for x in eval_pred_all],dtype=\"object\"),allow_pickle=True)\n",
    "#preds=np.load(\"../results/binary_abuse/zero_shot_prompt_engineering/preds.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to csv\n",
    "def load_file(dir, file_name):\n",
    "    with open(dir+file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    return df\n",
    "\n",
    "\n",
    "zero_shot_list = []\n",
    "folder=\"../results/binary_abuse/zero_shot_prompt_engineering/\"\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for filepath in os.listdir(folder):\n",
    "    if filepath.endswith((\".json\")):\n",
    "        print(filepath)\n",
    "        tempfile=load_file(folder,filepath)\n",
    "        zero_shot_list.append(tempfile)\n",
    "        \n",
    "results_df = pd.concat(zero_shot_list, axis = 1).T\n",
    "results_df[\"n_used\"]= results_df.eval_true.map(len)\n",
    "results_df[\"zero_shot_prompt\"] = prompt_list\n",
    "results_df.to_csv(\"../results/zero_shot_task_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
