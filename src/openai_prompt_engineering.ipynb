{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e9849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI Prompt Engineering\n",
    "# Date Created: 03/07/23\n",
    "\n",
    "# Load in packages\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import tiktoken\n",
    "from evaluation import evaluate, get_results_dict, save_results\n",
    "from helper_functions import check_dir_exists, load_n_samples, load_balanced_n_samples, convert_labels\n",
    "import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5131e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in datasets\n",
    "binary_abuse =pd.read_csv(\"../data/binary_abuse/clean_data/binary_abuse_dev_sample.csv\")\n",
    "imdb = pd.read_csv(\"../data/binary_movie_sentiment/clean_data/binary_movie_sentiment_dev_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://openai.com/pricing#language-models\n",
    "# GPT-3.5 Turbo (as of 18/07/2023)\n",
    "# 4K context   $0.0015 / 1K tokens  $0.002 / 1K tokens\n",
    "# 16K context  $0.003 / 1K tokens   $0.004 / 1K tokens\n",
    "\n",
    "# https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/?cdn=disable\n",
    "# Models           Per 1,000 tokens\n",
    "# gpt-3.5-turbo    Â£0.001586\n",
    "\n",
    "# iterate over all to get the number of tokens - imdb\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\") # shuold be the tokenizer for gpt3.5\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(imdb[\"text\"])):\n",
    "    sample_text=imdb[\"text\"][i]\n",
    "    decoded_text = tokenizer.decode_tokens_bytes(tokenizer.encode(sample_text))\n",
    "    token_lengths.append(len(decoded_text))\n",
    "\n",
    "print(\"IMDB: N=\",len(token_lengths),\"; Max Token Len=\",max(token_lengths),\"; Min Token Len=\",min(token_lengths),\"; Avg Token Len=\",sum(token_lengths)/len(token_lengths))\n",
    "print(\"\\n\")\n",
    "print(imdb[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "# IMDB: N= 1250 ; Max Token Len= 1407 ; Min Token Len= 8 ; Avg Token Len= 274.024\n",
    "\n",
    "\n",
    "# False    625\n",
    "# True     625\n",
    "# Name: label, dtype: int64\n",
    "# CPU times: total: 5.05 s\n",
    "# Wall time: 5.18 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# iterate over all to get the number of tokens -binary_abuse\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\") # shuold be the tokenizer for gpt3.5\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(binary_abuse[\"text\"])):\n",
    "    sample_text=binary_abuse[\"text\"][i]\n",
    "    decoded_text = tokenizer.decode_tokens_bytes(tokenizer.encode(sample_text))\n",
    "    token_lengths.append(len(decoded_text))\n",
    "\n",
    "print(\"Wiki Abuse: N=\",len(token_lengths),\"; Max Token Len=\",max(token_lengths),\"; Min Token Len=\",min(token_lengths),\"; Avg Token Len=\",sum(token_lengths)/len(token_lengths))\n",
    "print(\"\\n\")\n",
    "print(binary_abuse[\"label\"].value_counts())\n",
    "\n",
    "\n",
    "# Wiki Abuse: N= 2316 ; Max Token Len= 2858 ; Min Token Len= 2 ; Avg Token Len= 95.39896373056995\n",
    "\n",
    "\n",
    "# False    2041\n",
    "# True      275\n",
    "# Name: label, dtype: int64\n",
    "# CPU times: total: 3.61 s\n",
    "# Wall time: 3.66 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Access via text completion\n",
    "api_key= \"masked\"\n",
    "base_url = \"masked\"\n",
    "deployment_name =\"masked\" # cheat gpt text model for testing\n",
    "\n",
    "#deployment_name =\"tos_gpt35\" # gpt 3.5 final model for paper\n",
    "url = base_url + \"/openai/deployments/\" + deployment_name + \"/completions?api-version=2023-05-15\"\n",
    "\n",
    "\n",
    "def get_openai_response(prompt):\n",
    "    \"\"\"Gets a response from the OpenAI API from a given prompt\n",
    "    Args:\n",
    "        prompt: a text prompt to send to the API\n",
    "    Returns:\n",
    "        a string response from the API\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"prompt\":prompt,\n",
    "        \"max_tokens\":20,\n",
    "        \"top_p\":.1,\n",
    "        #\"stop\":[\" \",\".\",\",\"] # ideally only want one word responses\n",
    "        \"stop\":[\".\",\"\\n\",\",\",\"<\"] # get one sentence responses\n",
    "    }\n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.post(url,\n",
    "                              headers={\n",
    "                                  \"api-key\": api_key,\n",
    "                                  \"Content-Type\": \"application/json\"\n",
    "                              },\n",
    "                              json = payload,\n",
    "                              timeout=20)\n",
    "        except (requests.Timeout,ConnectionResetError,ConnectionError) as e: # this only work if the api response fails due to the API server hanging\n",
    "            print(e)\n",
    "            time.sleep(5) # if service is struggling, wait a few moments and try again - keep doing this until no timeout error\n",
    "            continue\n",
    "        break   \n",
    "    response = json.loads(r.text)\n",
    "    if r.status_code!=200:\n",
    "        #raise Exception(r.json().get(\"error\").get(\"message\"))\n",
    "        text_return = r.json().get(\"error\").get(\"message\")\n",
    "    \n",
    "    if r.status_code==200:\n",
    "        text_return = response['choices'][0]['text']\n",
    "        text_return = text_return.replace('\"', '') # remove \n",
    "        text_return = text_return.replace('\\'', '') # remove\n",
    "        text_return = text_return.strip()\n",
    "    \n",
    "        if text_return==\"Negative\" or text_return==\"Abusive\":\n",
    "            text_return=\"Yes\"        \n",
    "        if text_return==\"Not\" or text_return==\"Positive\":\n",
    "            text_return=\"No\"\n",
    "        \n",
    "    return text_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d080f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Access via chat - 18/07/23 switching to this as gpt3.5 turbo seems optimized for chat over completions\n",
    "\n",
    "openai.api_key = \"masked\"\n",
    "openai.api_base =\"masked\" \n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "deployment_name =\"masked\" \n",
    "\n",
    "def get_openai_response_chat(prompt, text):\n",
    "    \"\"\"Gets a response from the OpenAI API from a given prompt\n",
    "    Args:\n",
    "        prompt: a text prompt to send to the API\n",
    "    Returns:\n",
    "        a string response from the API\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=deployment_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": text}\n",
    "                ],\n",
    "                temperature=.1,\n",
    "                max_tokens=20,\n",
    "                stop=[\".\",\",\",\"<\"],\n",
    "                request_timeout=20, #\n",
    "            )\n",
    "        except (requests.Timeout,ConnectionResetError,ConnectionError,openai.error.ServiceUnavailableError,RemoteDisconnected,APIConnectionError) as e: # this only work if the api response fails due to the API server hanging\n",
    "            print(e)\n",
    "            time.sleep(10) # if service is struggling, wait a few moments and try again - keep doing this until no timeout error\n",
    "            continue\n",
    "        break \n",
    "        \n",
    "\n",
    "    \n",
    "    text_return = response['choices'][0]['message']['content']\n",
    "    \n",
    "    \n",
    "    return text_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempted prompts\n",
    "# p: \"\\n\\nDoes this text contain abuse, Yes or No?\\n\\n\"\n",
    "# r: 'I\\'m not sure what you mean by \"abuse'\n",
    "# p: \"\\n\\nDoes this text contain abusive language, Yes or No?\\n\\n\"\n",
    "# r: No\n",
    "# p: \"\\n\\nDoes this text contain abusive language? Respond with either Yes or No.\\n\\n\"\n",
    "# r: 'No'\n",
    "# p: \\n\\nIs this text abusive, Yes or No?\\n\\n\n",
    "# r: I think this question violates the Terms of Service\n",
    "# p: \\n\\nIs this text using abusive language, Yes or No?\\n\\n\n",
    "# r: 'No'\n",
    "# p: \\n\\nIt was? Abusive or Not Abusive?\\n\\n\n",
    "# r: \"I'm not sure what you're asking\"\n",
    "# p: \\n\\nIt was? Abusive Language or Not Abusive Language?\\n\\n\n",
    "# r: I'm not sure what you're askin\n",
    "# p: \\n\\nDoes the above user comment contain abuse? Yes or No. \\n\\n\n",
    "# r: 'Posted by: Cato | 2005'\n",
    "# p: \"\\n\\nDoes the above user comment contain abuse, Yes or No? \\n\\n\"\n",
    "# r: 'No\n",
    "# p: \\n\\nAbusive or Not Abusive? \\n\\n\n",
    "# r: 'Posted by: The Brickmuppet at 11:47 PM | Comments (1) | Add'\n",
    "# p: \\n\\nAbusive or Not Abusive? Which better characterizes the above comment?\\n\\n\n",
    "# r: I'm not sure what you're asking\n",
    "# p: Was that comment Abusive or Not Abusive?\n",
    "# r: Enter your comment here\n",
    "# p: \\n\\nDoes the above user comment contain abusive language, Yes or No? \\n\\n\n",
    "# r: No\n",
    "# p: You are a researcher evaluating abusive language in Wikipedia comments. Does the above comment contain abusive language, Yes or No? \n",
    "# r: 'No'\n",
    "\n",
    "# p:\\n\\nDoes this text contain negative sentiment, Yes or No?\\n\\n\n",
    "# r: No\n",
    "# p:\\n\\nDoes this text contain negative sentiment, Yes or No? \\n\\n\n",
    "# r: Yes\n",
    "# p:\\n\\nDoes the above text contain negative sentiment, Yes or No? \\n\\n\n",
    "# r: Yes\n",
    "# p: \\n\\nIs this text negative, Yes or No?\\n\\n\n",
    "# r:Was the above review useful to you?\n",
    "# p: \\n\\nIt was? Negative or Not Negative?\\n\\n\n",
    "# r: I'm not sure what to say about this movie\n",
    "# p:\\n\\nDoes this text contain negative sentiment, Yes or No?\\n\\n\n",
    "# r: \\n\\nClassify the sentiment of the above text using Positive or Negative.\\n\\nThe above text is \n",
    "# p: ' Negative'\n",
    "# r: \\n\\nWhat is the sentiment of the above text using Positive or Negative.\\n\\nThe above text is \n",
    "# p: ' Negative'\n",
    "# r: \"Using Positive or Negative, classify the sentiment of the following movie review.\\n\\n .... \\n\\nThe sentiment of the movie review is\n",
    "# p:' Negative'\n",
    "\n",
    "\n",
    "binary_abuse_prompt_1 = \"\\n\\nDoes this text contain abuse, Yes or No?\\n\\n\"\n",
    "sample_text=imdb[\"text\"][1]\n",
    "prompt = sample_text + binary_abuse_prompt_2\n",
    "print(prompt)\n",
    "\n",
    "get_openai_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbf2e1",
   "metadata": {},
   "source": [
    "# Final Zero Shot Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set global vars\n",
    "TECH = \"zero_shot_prompt_engineering\"\n",
    "TASK = \"binary_abuse\"\n",
    "\n",
    "n_eval=-1\n",
    "eval_set=\"dev_sample\"\n",
    "model_name = \"gpt4\"\n",
    "run_time = \"0\" # no training\n",
    "n_train= \"0\" # no training\n",
    "seed= \"0\" # no training\n",
    "balanced_train=False\n",
    "\n",
    "# Set dirs\n",
    "main_dir = os.getcwd()\n",
    "data_dir = f\"../data\"\n",
    "output_dir = f'../results/{TASK}/{TECH}'\n",
    "\n",
    "# TASK = \"binary_abuse\"\n",
    "# True = Abusive; False = Not Abusive\n",
    "binary_abuse_prompt_1 = \"Using one word, does the internet comment contain toxic language, Yes or No?\"\n",
    "binary_abuse_prompt_2 = \"Using one word, is this internet comment using toxic language, Yes or No?\"\n",
    "binary_abuse_prompt_3 = \"You are a researcher who needs to classify comments on the internet as containing abusive language or not containing abusive language. Using one word, does the internet comment contain abusive language, Yes or No?\"\n",
    " \n",
    "# TASK = \"binary_movie_sentiment\"\n",
    "# True= Positive; False= Negative\n",
    "imdb_prompt_1 = \"Using one word, does the movie review contain negative sentiment, Yes or No?\"\n",
    "imdb_prompt_2 = \"Using one word, classify the sentiment of the movie review using 'Positive' or 'Negative'.\"\n",
    "imdb_prompt_3 = \"You are a researcher who needs to classify movie reviews as containing negative sentiment or not containing negative sentiment. Using one word, does the movie review contain negative sentiment, Yes or No?\"\n",
    "\n",
    "# Lists for loop\n",
    "task_list = [\"binary_abuse\",\"binary_abuse\",\"binary_abuse\",\"binary_movie_sentiment\",\"binary_movie_sentiment\",\"binary_movie_sentiment\"]\n",
    "prompt_list = [binary_abuse_prompt_1,binary_abuse_prompt_2,binary_abuse_prompt_3,imdb_prompt_1,imdb_prompt_2,imdb_prompt_3]\n",
    "\n",
    "\n",
    "####################################################################\n",
    "eval_pred_all = [] # the master lists\n",
    "eval_gold_all = [] # the master lists\n",
    "for t in range(1,6):\n",
    "    task_prompt = prompt_list[t]\n",
    "    TASK = task_list[t]\n",
    "    temp_df = binary_abuse\n",
    "    if TASK == \"binary_movie_sentiment\":\n",
    "        temp_df=imdb\n",
    "    # iterate over the rows\n",
    "    #for i in range(len(imdb[\"text\"])):\n",
    "    eval_gold_labels=[] # the t/p lists\n",
    "    eval_pred=[] # the t/p lists\n",
    "    for i in range(len(temp_df)):\n",
    "        # grab the comment\n",
    "        sample_text=temp_df[\"text\"][i]\n",
    "        \n",
    "        # make it into a usable user chat\n",
    "        if TASK == \"binary_movie_sentiment\":\n",
    "            temp_text = \"Movie review: \" + sample_text\n",
    "            \n",
    "        if TASK == \"binary_abuse\":\n",
    "            temp_text = \"Internet comment: \" + sample_text\n",
    "        \n",
    "        # for gpt 3 only since we can't use the chat api\n",
    "        #temp_prompt = temp_text + \"\\nQuestion: \"+ task_prompt +\" \\nResponse:\"\n",
    "        #eval_pred.append(get_openai_response(temp_prompt)) # Don't think i need the below\n",
    "        \n",
    "        # append gpt response to list\n",
    "        eval_pred.append(get_openai_response_chat(task_prompt,temp_text))       \n",
    "        \n",
    "        # append labels to list\n",
    "        eval_gold_labels.append(temp_df[\"label\"][i])\n",
    "        \n",
    "        time.sleep(2) # we should be fine vis-a-vis rate limits, but adding a small delay to help prevent any issues with ServiceUnavailableErrors\n",
    "        if i % 50 == 0:\n",
    "            print(t,i)\n",
    "    \n",
    "    eval_pred_all.append(eval_pred)\n",
    "    \n",
    "    # Need to remember that these are flipped \n",
    "    if TASK == \"binary_abuse\":\n",
    "        eval_pred_int=list(pd.Series(eval_pred).map(dict(Yes=1, No=0)))\n",
    "    \n",
    "    # switch my pos/neg to match the other imdb prompts\n",
    "    if task_prompt == imdb_prompt_2:\n",
    "        eval_pred=list(pd.Series(eval_pred).map(dict(Negative=\"Yes\", Positive=\"No\")))\n",
    "        \n",
    "    if TASK == \"binary_movie_sentiment\":\n",
    "        eval_pred_int=list(pd.Series(eval_pred).map(dict(Yes=0, No=1)))\n",
    "    \n",
    "    # Deal with non yes/no answers\n",
    "    print(\"Number of improperly coded responses: \",sum(math.isnan(x) for x in eval_pred_int))\n",
    "    \n",
    "    cleaned_eval_pred_int = [x for x in eval_pred_int if not math.isnan(x)]\n",
    "    cleaned_eval_gold_labels = [eval_gold_labels[i] for i, x in enumerate(eval_pred_int) if not math.isnan(x)]\n",
    "    \n",
    "    \n",
    "    cleaned_eval_gold_labels=list(map(int,cleaned_eval_gold_labels))\n",
    "    \n",
    "    eval_gold_all.append(cleaned_eval_gold_labels)\n",
    "    \n",
    "    results = evaluate(cleaned_eval_gold_labels, cleaned_eval_pred_int)\n",
    "    \n",
    "\n",
    "    print(TASK)\n",
    "    print(task_prompt)\n",
    "    print(results)\n",
    "    \n",
    "    eval_gold_labels=cleaned_eval_gold_labels\n",
    "    eval_preds=cleaned_eval_pred_int\n",
    "    eval_result=results\n",
    "    template=task_prompt\n",
    "\n",
    "    \n",
    "    datetime_str = str(datetime.datetime.now())\n",
    "    results_dict = get_results_dict(TASK, TECH, model_name, run_time,\n",
    "                    eval_gold_labels, eval_preds, eval_set,\n",
    "                    n_train, n_eval, balanced_train, seed, datetime_str, template)\n",
    "    # add test_result to results_dict\n",
    "    results_dict.update(eval_result)\n",
    "    save_str = f'mod={model_name}_n={n_train}_bal={balanced_train}_iteration={t}'\n",
    "    save_results(output_dir, save_str, results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of Non-Response/Incorrect Response\n",
    "\n",
    "# I cannot determine if the internet comment contains toxic language\n",
    "# I'm sorry, but I cannot determine whether the internet comment contains toxic language without the actual comment\n",
    "# N/A (not a comment)\n",
    "# I cannot determine if the internet comment contains toxic language as it appears to be incomplete and lacks context\n",
    "# Cannot determine as there is no internet comment provided\n",
    "# I cannot determine if the internet comment contains toxic language\n",
    "# Personal attack\n",
    "# I cannot make a judgement without seeing the actual internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39578b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save/Load the GPT predictions\n",
    "#np.save(\"../results/binary_abuse/zero_shot_prompt_engineering/preds_2.npy\",np.array([np.arrSay(x, dtype=\"object\") for x in eval_pred_all],dtype=\"object\"),allow_pickle=True)\n",
    "#preds=np.load(\"../results/binary_abuse/zero_shot_prompt_engineering/preds_2.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to csv\n",
    "def load_file(dir, file_name):\n",
    "    with open(dir+file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    return df\n",
    "\n",
    "\n",
    "zero_shot_list = []\n",
    "folder=\"../results/binary_abuse/zero_shot_prompt_engineering/\"\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for filepath in os.listdir(folder):\n",
    "#    if filepath.endswith((\".json\")):\n",
    "    if filepath.startswith((\"mod=gpt4\")):    \n",
    "        print(filepath)\n",
    "        tempfile=load_file(folder,filepath)\n",
    "        zero_shot_list.append(tempfile)\n",
    "        \n",
    "results_df = pd.concat(zero_shot_list, axis = 1).T\n",
    "results_df[\"n_used\"]= results_df.eval_true.map(len)\n",
    "results_df[\"zero_shot_prompt\"] = prompt_list\n",
    "#results_df.to_csv(\"../results/zero_shot_task_results_gpt4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09667c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the saved GPT predictions to see why non-response answers occur\n",
    "binary_abuse[\"p1\"] = preds[0]\n",
    "binary_abuse[\"p2\"] = preds[1]\n",
    "binary_abuse[\"p3\"] = preds[2]\n",
    "# imdb[\"p1\"] = preds[1]\n",
    "# imdb[\"p2\"] = preds[2]\n",
    "# imdb[\"p3\"] = preds[3]\n",
    "\n",
    "# Binary Abuse = \n",
    "binary_abuse.loc[(binary_abuse['p3'] != \"Yes\") & (binary_abuse['p3'] != \"No\")][[\"p3\"]].values\n",
    "\n",
    "# Wiki Toxic Review = all different cases where \n",
    "print(binary_abuse.loc[(binary_abuse['p1'] != \"Yes\") & (binary_abuse['p1'] != \"No\")][[\"p1\",\"p2\",\"p3\"]])\n",
    "print(binary_abuse.loc[(binary_abuse['p2'] != \"Yes\") & (binary_abuse['p2'] != \"No\")][[\"p1\",\"p2\",\"p3\"]])\n",
    "print(binary_abuse.loc[(binary_abuse['p3'] != \"Yes\") & (binary_abuse['p3'] != \"No\")][[\"p1\",\"p2\",\"p3\"]])\n",
    "\n",
    "# array([[\"I'm sorry\"],\n",
    "#        ['Cannot determine if the internet comment contains abusive language or not based on the given text'],\n",
    "#        ['Abusive language: No'],\n",
    "#        [\"I'm sorry\"],\n",
    "#        ['Cannot determine if the internet comment contains abusive language or not as it does not contain any language'],\n",
    "#        ['I cannot determine if this internet comment contains abusive language or not as it is not a complete sentence and'],\n",
    "#        ['The given internet comment does not contain any language'],\n",
    "#        ['The internet comment does not contain a clear indication of abusive language'],\n",
    "#        ['I cannot determine whether this internet comment contains abusive language or not as it does not contain any text'],\n",
    "#        [\"I'm sorry\"],\n",
    "#        ['Sorry'],\n",
    "#        ['I cannot determine if the internet comment contains abusive language or not as the comment is incomplete and ends abruptly'],\n",
    "#        ['I cannot determine if the internet comment contains abusive language or not as it is incomplete and does not contain'],\n",
    "#        [\"I'm sorry\"],\n",
    "#        ['Cannot determine if the internet comment contains abusive language or not based on the given text'],\n",
    "#        [\"I'm sorry\"],\n",
    "#        [\"I'm sorry\"],\n",
    "#        ['Sorry'],\n",
    "#        [\"I'm sorry\"]], dtype=object)\n",
    "\n",
    "# p1     p2      p3\n",
    "# 52     52      18    \n",
    "# 474    245     52    \n",
    "# 476    474     245   \n",
    "# 519    553     474   \n",
    "# 534    756     491   \n",
    "# 719    998     519   \n",
    "# 756    1005    741   \n",
    "# 975    1162    756   \n",
    "# 998    1240    914   \n",
    "# 1005   1254    998   \n",
    "# 1111   1550    1005  \n",
    "# 1240   1608    1024  \n",
    "# 1254   1992    1162  \n",
    "# 1290           1202  \n",
    "# 1420           1290  \n",
    "# 1550           1717  \n",
    "# 1608           1825  \n",
    "# 1683           1836  \n",
    "# 1730           2090 \n",
    "# 1825          \n",
    "# 1992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bfefae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie Review = all different cases where \n",
    "print(imdb.loc[(imdb['p1'] != \"Yes\") & (imdb['p1'] != \"No\")][[\"p1\",\"p2\",\"p3\"]])\n",
    "print(imdb.loc[(imdb['p2'] != \"Negative\") & (imdb['p2'] != \"Positive\")][[\"p1\",\"p2\",\"p3\"]])\n",
    "print(imdb.loc[(imdb['p3'] != \"Yes\") & (imdb['p3'] != \"No\")][[\"p1\",\"p2\",\"p3\"]])\n",
    "\n",
    "\n",
    "#  p1    p2    p3   \n",
    "# 367   230   348  \n",
    "# 1070  417   496  \n",
    "#       760   578  \n",
    "#       868   592  \n",
    "#       984   1057  \n",
    "#       1009  1144  \n",
    "#       1035  1165  \n",
    "#       1087 \n",
    "\n",
    "# p1       p2  p3\n",
    "# No  Neutral  No\n",
    "# No    Mixed  No"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6e932",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in datasets\n",
    "tmdb = pd.read_csv(\"../data/tmdb/movie_sentiment_tmdb.csv\")\n",
    "tmdb[\"rating_int\"] = np.where(tmdb['binary_rating']=='Positive', 1, 0) # pos =1, neg = 0\n",
    "tmdb.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all to get the number of tokens - imdb\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\") # shuold be the tokenizer for gpt3.5\n",
    "token_lengths = []\n",
    "\n",
    "for i in range(len(tmdb[\"reviews\"])):\n",
    "    sample_text=tmdb[\"reviews\"][i]\n",
    "    decoded_text = tokenizer.decode_tokens_bytes(tokenizer.encode(sample_text))\n",
    "    token_lengths.append(len(decoded_text))\n",
    "\n",
    "print(\"TMDB: N=\",len(token_lengths),\"; Max Token Len=\",max(token_lengths),\"; Min Token Len=\",min(token_lengths),\"; Avg Token Len=\",sum(token_lengths)/len(token_lengths))\n",
    "print(\"\\n\")\n",
    "print(tmdb[\"binary_rating\"].value_counts())\n",
    "\n",
    "\n",
    "# TMDB: N= 855 ; Max Token Len= 1861 ; Min Token Len= 2 ; Avg Token Len= 211.87251461988305\n",
    "\n",
    "\n",
    "# Positive    627\n",
    "# Negative    228\n",
    "# Name: binary_rating, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86955c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set global vars\n",
    "TECH = \"zero_shot_prompt_engineering\"\n",
    "TASK = \"binary_movie_sentiment_sensitivity\"\n",
    "\n",
    "n_eval=-1\n",
    "eval_set=\"dev_sample\"\n",
    "model_name = \"gpt3.0\"\n",
    "run_time = \"0\" # no training\n",
    "n_train= \"0\" # no training\n",
    "seed= \"0\" # no training\n",
    "balanced_train=False\n",
    "\n",
    "# Set dirs\n",
    "main_dir = os.getcwd()\n",
    "data_dir = f\"../data\"\n",
    "output_dir = f'../results/{TASK}/{TECH}'\n",
    "\n",
    "# TASK = \"binary_movie_sentiment\"\n",
    "# True= Positive; False= Negative\n",
    "tmdb_prompt_1 = \"Using one word, does the movie review contain negative sentiment, Yes or No?\"\n",
    "tmdb_prompt_2 = \"Using one word, classify the sentiment of the movie review using 'Positive' or 'Negative'.\"\n",
    "tmdb_prompt_3 = \"You are a researcher who needs to classify movie reviews as containing negative sentiment or not containing negative sentiment. Using one word, does the movie review contain negative sentiment, Yes or No?\"\n",
    "\n",
    "# Lists for loop\n",
    "task_list = [\"binary_movie_sentiment_sensitivity\",\"binary_movie_sentiment_sensitivity\",\"binary_movie_sentiment_sensitivity\"]\n",
    "prompt_list = [tmdb_prompt_1,tmdb_prompt_2,tmdb_prompt_3]\n",
    "model_list = [\"gpt3.0\",\"gpt3.5\",\"gpt4.0\"]\n",
    "\n",
    "####################################################################\n",
    "eval_pred_all = [] # the master lists\n",
    "eval_gold_all = [] # the master lists\n",
    "\n",
    "# iterate over the three models\n",
    "for m in range(3):\n",
    "    model_name = model_list[m]\n",
    "    # set up api settings - 3.0 uses a different system so don't need these for 3.0\n",
    "    if model_name==\"gpt3.5\":\n",
    "        openai.api_key = \"masked\"\n",
    "        openai.api_base =\"masked\" # gpt 3,3.5\n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_version = \"2023-03-15-preview\"\n",
    "        deployment_name =\"masked\" \n",
    "    if model_name==\"gpt4.0\":\n",
    "        openai.api_key = \"masked\"\n",
    "        openai.api_base = \"masked\n",
    "        deployment_name =\"masked\" \n",
    "        openai.api_type = \"azure\"\n",
    "        openai.api_version = \"2023-03-15-preview\"\n",
    "    # iterate over the three prompts   \n",
    "    for t in range(3):\n",
    "        task_prompt = prompt_list[t]\n",
    "        TASK = task_list[t]\n",
    "        temp_df = tmdb\n",
    "        \n",
    "        # iterate over the rows\n",
    "        eval_gold_labels=[] # the t/p lists\n",
    "        eval_pred=[] # the t/p lists\n",
    "        for i in range(len(temp_df)):\n",
    "            # grab the comment\n",
    "            sample_text=temp_df[\"reviews\"][i]\n",
    "            \n",
    "            # make it into a usable user chat\n",
    "            temp_text = \"Movie review: \" + sample_text\n",
    "            \n",
    "            if model_name==\"gpt3.0\":\n",
    "                # for gpt 3 only since we can't use the chat api\n",
    "                temp_prompt = temp_text + \"\\nQuestion: \"+ task_prompt +\" \\nResponse:\"\n",
    "                eval_pred.append(get_openai_response(temp_prompt)) # Don't think i need the below\n",
    "            \n",
    "            if model_name!=\"gpt3.0\":\n",
    "                # append gpt response to list\n",
    "                eval_pred.append(get_openai_response_chat(task_prompt,temp_text))       \n",
    "            \n",
    "            # append labels to list\n",
    "            eval_gold_labels.append(temp_df[\"rating_int\"][i])\n",
    "            \n",
    "            time.sleep(1) # we should be fine vis-a-vis rate limits, but adding a small delay to help prevent any issues with ServiceUnavailableErrors\n",
    "            if i % 50 == 0:\n",
    "                print(t,i)\n",
    "        \n",
    "        eval_pred_all.append(eval_pred)\n",
    "        \n",
    "        # Need to remember that these are flipped \n",
    "        \n",
    "        # switch my pos/neg to match the other imdb prompts\n",
    "        if task_prompt == tmdb_prompt_2:\n",
    "            eval_pred=list(pd.Series(eval_pred).map(dict(Negative=\"Yes\", Positive=\"No\")))\n",
    "            \n",
    "        if TASK == \"binary_movie_sentiment_sensitivity\":\n",
    "            eval_pred_int=list(pd.Series(eval_pred).map(dict(Yes=0, No=1)))\n",
    "        \n",
    "        # Deal with non yes/no answers\n",
    "        print(\"Number of improperly coded responses: \",sum(math.isnan(x) for x in eval_pred_int))\n",
    "        \n",
    "        cleaned_eval_pred_int = [x for x in eval_pred_int if not math.isnan(x)]\n",
    "        cleaned_eval_gold_labels = [eval_gold_labels[i] for i, x in enumerate(eval_pred_int) if not math.isnan(x)]\n",
    "        \n",
    "        \n",
    "        cleaned_eval_gold_labels=list(map(int,cleaned_eval_gold_labels))\n",
    "        \n",
    "        eval_gold_all.append(cleaned_eval_gold_labels)\n",
    "        \n",
    "        results = evaluate(cleaned_eval_gold_labels, cleaned_eval_pred_int)\n",
    "        \n",
    "    \n",
    "        print(TASK)\n",
    "        print(task_prompt)\n",
    "        print(results)\n",
    "        \n",
    "        eval_gold_labels=cleaned_eval_gold_labels\n",
    "        eval_preds=cleaned_eval_pred_int\n",
    "        eval_result=results\n",
    "        template=task_prompt\n",
    "    \n",
    "        \n",
    "        datetime_str = str(datetime.datetime.now())\n",
    "        results_dict = get_results_dict(TASK, TECH, model_name, run_time,\n",
    "                        eval_gold_labels, eval_preds, eval_set,\n",
    "                        n_train, n_eval, balanced_train, seed, datetime_str, template)\n",
    "        # add test_result to results_dict\n",
    "        results_dict.update(eval_result)\n",
    "        save_str = f'mod={model_name}_n={n_train}_bal={balanced_train}_iteration={t}'\n",
    "        save_results(output_dir, save_str, results_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deb7844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to csv\n",
    "def load_file(dir, file_name):\n",
    "    with open(dir+file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    return df\n",
    "\n",
    "\n",
    "zero_shot_list = []\n",
    "folder=\"../results/binary_movie_sentiment_sensitivity/zero_shot_prompt_engineering/\"\n",
    "\n",
    "# Load the images, and append them to a list.\n",
    "for filepath in os.listdir(folder):\n",
    "#    if filepath.endswith((\".json\")):\n",
    "    if filepath.startswith((\"mod\")):     # include all\n",
    "        print(filepath)\n",
    "        tempfile=load_file(folder,filepath)\n",
    "        zero_shot_list.append(tempfile)\n",
    "        \n",
    "results_df = pd.concat(zero_shot_list, axis = 1).T\n",
    "results_df[\"n_used\"]= results_df.eval_true.map(len)\n",
    "results_df[\"zero_shot_prompt\"] = prompt_list*3\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to recalculate F1 Scores using the saved arrays\n",
    "\n",
    "from evaluation import evaluate_dataframe\n",
    "results_df.index = pd.RangeIndex(start=0, step=1, stop=len(results_df))\n",
    "# results_df['eval_true'] = results_df['eval_true'].apply(ast.literal_eval)\n",
    "# results_df['eval_pred'] = results_df['eval_pred'].apply(ast.literal_eval)\n",
    "# Evaluate column scores\n",
    "results_df[['acc_new', 'f1_new', 'prec_new', 'recall_new']] = results_df.apply(evaluate_dataframe, axis=1, result_type=\"expand\")\n",
    "results_df.head()\n",
    "#results_df.to_csv(\"../results/zero_shot_task_results_sensitvity.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
